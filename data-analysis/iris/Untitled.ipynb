{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/iris.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/iris.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.028558 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.028558 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[float,float,float,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/iris.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/iris.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 150 lines in 0.010187 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 150 lines in 0.010187 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sf=graphlab.SFrame('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canvas is accessible via web browser at the URL: http://localhost:49715/index.html\n",
      "Opening Canvas in default web browser.\n"
     ]
    }
   ],
   "source": [
    "sf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">5.1</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">3.5</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">1.4</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">0.2</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Iris-setosa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.7</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.6</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.6</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.7</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.6</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.7</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Iris-setosa</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[149 rows x 5 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\t5.1\tfloat\n",
       "\t3.5\tfloat\n",
       "\t1.4\tfloat\n",
       "\t0.2\tfloat\n",
       "\tIris-setosa\tstr\n",
       "\n",
       "Rows: 149\n",
       "\n",
       "Data:\n",
       "+-----+-----+-----+-----+-------------+\n",
       "| 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa |\n",
       "+-----+-----+-----+-----+-------------+\n",
       "| 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa |\n",
       "| 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa |\n",
       "| 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 5.4 | 3.9 | 1.7 | 0.4 | Iris-setosa |\n",
       "| 4.6 | 3.4 | 1.4 | 0.3 | Iris-setosa |\n",
       "| 5.0 | 3.4 | 1.5 | 0.2 | Iris-setosa |\n",
       "| 4.4 | 2.9 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 4.9 | 3.1 | 1.5 | 0.1 | Iris-setosa |\n",
       "| 5.4 | 3.7 | 1.5 | 0.2 | Iris-setosa |\n",
       "+-----+-----+-----+-----+-------------+\n",
       "[149 rows x 5 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: str\n",
       "Rows: 150\n",
       "['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', ... ]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf['1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.843333333333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf['1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method mean in module graphlab.data_structures.sarray:\n",
      "\n",
      "mean(self) method of graphlab.data_structures.sarray.SArray instance\n",
      "    Mean of all the values in the SArray, or mean image.\n",
      "    \n",
      "    Returns None on an empty SArray. Raises an exception if called on an\n",
      "    SArray with non-numeric type or non-Image type.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : float | graphlab.Image\n",
      "        Mean of all values in SArray, or image holding per-pixel mean\n",
      "        across the input SArray.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sf['1'].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1475240028.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to chenye277@126.com and will expire on September 08, 2017.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/test.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/test.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1 lines in 0.022967 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1 lines in 0.022967 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/test.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/Crayon_277/Develop/Project/Python/data-analysis/iris/test.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1 lines in 0.009329 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1 lines in 0.009329 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = graphlab.SFrame('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">id age gender education<br>query_list ...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">00627779E16E7C09B975B2CE1<br>3C088CB     4     2   ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[1 rows x 1 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tid age gender education query_list\tstr\n",
       "\n",
       "Rows: 1\n",
       "\n",
       "Data:\n",
       "+-------------------------------+\n",
       "| id age gender education qu... |\n",
       "+-------------------------------+\n",
       "| 00627779E16E7C09B975B2CE13... |\n",
       "+-------------------------------+\n",
       "[1 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('test.csv','r') as fr, open('test_after.csv','w') as fw:\n",
    "        fw.write('id,age,gender,education,query_list\\n')\n",
    "        for i in fr:\n",
    "            l = i.split()\n",
    "            fw.write('{0},{1},{2},{3},{4}\\n'.format(l.pop(0),l.pop(0),l.pop(0),l.pop(0),l))\n",
    "except:\n",
    "    print 'file error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitWord(words):\n",
    "    uni = words.decode('utf-8')\n",
    "    li = list()    \n",
    "    for u in uni:\n",
    "        li.append(u.encode('utf-8'))\n",
    "    return li   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(sys) \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('test.txt','r') as rr,open('test_a.txt','w') as ww:\n",
    "        for i in rr:\n",
    "            test_line = i.split(',')\n",
    "            print test_line\n",
    "            #line_test = []\n",
    "            #for word in test_line:\n",
    "            #    uni = word.decode('utf-8')\n",
    "            #    line_test.append(uni.encode('utf-8'))\n",
    "            ww.write('{}\\n'.format(test_line))\n",
    "except IOError as e:\n",
    "    print 'error',e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = open('test.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_line = rr.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe5\\x85\\xb3\\xe9\\x94\\xae\\xe6\\x80\\xa7,\\xe8\\xbf\\x99\\xe4\\xba\\x8b\\xe4\\xbb\\x80\\xe4\\xb9\\x88,\\xe6\\x88\\x91\\xe7\\x9a\\x84\\xe5\\xa4\\xa9\\xe5\\x93\\xaa\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print test_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe5\\x85\\xb3\\xe9\\x94\\xae\\xe6\\x80\\xa7,\\xe8\\xbf\\x99\\xe4\\xba\\x8b\\xe4\\xbb\\x80\\xe4\\xb9\\x88,\\xe6\\x88\\x91\\xe7\\x9a\\x84\\xe5\\xa4\\xa9\\xe5\\x93\\xaa\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print test_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\u5173\\u952e\\u6027,\\u8fd9\\u4e8b\\u4ec0\\u4e48,\\u6211\\u7684\\u5929\\u54ea\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_line.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xb9\\xd8\\xbc\\xfc\\xd0\\xd4,\\xd5\\xe2\\xca\\xc2\\xca\\xb2\\xc3\\xb4,\\xce\\xd2\\xb5\\xc4\\xcc\\xec\\xc4\\xc4\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_line.encode('gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe5\\x85\\xb3\\xe9\\x94\\xae\\xe6\\x80\\xa7,\\xe8\\xbf\\x99\\xe4\\xba\\x8b\\xe4\\xbb\\x80\\xe4\\xb9\\x88,\\xe6\\x88\\x91\\xe7\\x9a\\x84\\xe5\\xa4\\xa9\\xe5\\x93\\xaa\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-6380e56b0626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0},{1},{2},{3},{4}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplitWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'file error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-5b50ab2871c4>\u001b[0m in \u001b[0;36msplitWord\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplitWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('test.csv','r') as fr, open('test_after.csv','w') as fw:\n",
    "        fw.write('id,age,gender,education,query_list\\n')\n",
    "        for i in fr:\n",
    "            l = i.split()\n",
    "            fw.write('{0},{1},{2},{3},{4}\\n'.format(l.pop(0),l.pop(0),l.pop(0),l.pop(0),splitWord(l)))\n",
    "except IOError as e:\n",
    "    print 'file error',e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=['关键性','这事什么']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\xe5\\x85\\xb3\\xe9\\x94\\xae\\xe6\\x80\\xa7',\n",
       " '\\xe8\\xbf\\x99\\xe4\\xba\\x8b\\xe4\\xbb\\x80\\xe4\\xb9\\x88']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我的\n"
     ]
    }
   ],
   "source": [
    "string = '我的'\n",
    "print string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=['关键性','这事什么']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\xe5\\x85\\xb3\\xe9\\x94\\xae\\xe6\\x80\\xa7',\n",
       " '\\xe8\\xbf\\x99\\xe4\\xba\\x8b\\xe4\\xbb\\x80\\xe4\\xb9\\x88']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe5\\x85\\xb3\\xe9\\x94\\xae\\xe6\\x80\\xa7'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键性\n"
     ]
    }
   ],
   "source": [
    "print t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jieba:\n",
      "\n",
      "NAME\n",
      "    jieba\n",
      "\n",
      "FILE\n",
      "    /Users/Crayon_277/anaconda2/envs/gl-env/lib/python2.7/site-packages/jieba/__init__.py\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    _compat\n",
      "    analyse (package)\n",
      "    finalseg (package)\n",
      "    posseg (package)\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        Tokenizer\n",
      "    \n",
      "    class Tokenizer(__builtin__.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dictionary=None)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  add_word(self, word, freq=None, tag=None)\n",
      "     |      Add a word to dictionary.\n",
      "     |      \n",
      "     |      freq and tag can be omitted, freq defaults to be a calculated value\n",
      "     |      that ensures the word can be cut out.\n",
      "     |  \n",
      "     |  calc(self, sentence, DAG, route)\n",
      "     |  \n",
      "     |  check_initialized(self)\n",
      "     |  \n",
      "     |  cut(self, sentence, cut_all=False, HMM=True)\n",
      "     |      The main function that segments an entire sentence that contains\n",
      "     |      Chinese characters into seperated words.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: The str(unicode) to be segmented.\n",
      "     |          - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "     |          - HMM: Whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  cut_for_search(self, sentence, HMM=True)\n",
      "     |      Finer segmentation for search engines.\n",
      "     |  \n",
      "     |  del_word(self, word)\n",
      "     |      Convenient function for deleting a word.\n",
      "     |  \n",
      "     |  gen_pfdict(self, f)\n",
      "     |  \n",
      "     |  get_DAG(self, sentence)\n",
      "     |  \n",
      "     |  get_dict_file(self)\n",
      "     |  \n",
      "     |  initialize(self, dictionary=None)\n",
      "     |  \n",
      "     |  lcut(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  lcut_for_search(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  load_userdict(self, f)\n",
      "     |      Load personalized dict to improve detect rate.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - f : A plain text file contains words and their ocurrences.\n",
      "     |                Can be a file-like object, or the path of the dictionary file,\n",
      "     |                whose encoding must be utf-8.\n",
      "     |      \n",
      "     |      Structure of dict file:\n",
      "     |      word1 freq1 word_type1\n",
      "     |      word2 freq2 word_type2\n",
      "     |      ...\n",
      "     |      Word type may be ignored\n",
      "     |  \n",
      "     |  set_dictionary(self, dictionary_path)\n",
      "     |  \n",
      "     |  suggest_freq(self, segment, tune=False)\n",
      "     |      Suggest word frequency to force the characters in a word to be\n",
      "     |      joined or splitted.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - segment : The segments that the word is expected to be cut into,\n",
      "     |                      If the word should be treated as a whole, use a str.\n",
      "     |          - tune : If True, tune the word frequency.\n",
      "     |      \n",
      "     |      Note that HMM may affect the final result. If the result doesn't change,\n",
      "     |      set HMM=False.\n",
      "     |  \n",
      "     |  tokenize(self, unicode_sentence, mode=u'default', HMM=True)\n",
      "     |      Tokenize a sentence and yields tuples of (word, start, end)\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: the str(unicode) to be segmented.\n",
      "     |          - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "     |          - HMM: whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    add_word(self, word, freq=None, tag=None) method of Tokenizer instance\n",
      "        Add a word to dictionary.\n",
      "        \n",
      "        freq and tag can be omitted, freq defaults to be a calculated value\n",
      "        that ensures the word can be cut out.\n",
      "    \n",
      "    calc(self, sentence, DAG, route) method of Tokenizer instance\n",
      "    \n",
      "    cut(self, sentence, cut_all=False, HMM=True) method of Tokenizer instance\n",
      "        The main function that segments an entire sentence that contains\n",
      "        Chinese characters into seperated words.\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: The str(unicode) to be segmented.\n",
      "            - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "            - HMM: Whether to use the Hidden Markov Model.\n",
      "    \n",
      "    cut_for_search(self, sentence, HMM=True) method of Tokenizer instance\n",
      "        Finer segmentation for search engines.\n",
      "    \n",
      "    del_word(self, word) method of Tokenizer instance\n",
      "        Convenient function for deleting a word.\n",
      "    \n",
      "    disable_parallel()\n",
      "    \n",
      "    enable_parallel(processnum=None)\n",
      "        Change the module's `cut` and `cut_for_search` functions to the\n",
      "        parallel version.\n",
      "        \n",
      "        Note that this only works using dt, custom Tokenizer\n",
      "        instances are not supported.\n",
      "    \n",
      "    get_DAG(self, sentence) method of Tokenizer instance\n",
      "    \n",
      "    get_FREQ lambda k, d=None\n",
      "    \n",
      "    get_dict_file(self) method of Tokenizer instance\n",
      "    \n",
      "    initialize(self, dictionary=None) method of Tokenizer instance\n",
      "    \n",
      "    lcut(self, *args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    lcut_for_search(self, *args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    load_userdict(self, f) method of Tokenizer instance\n",
      "        Load personalized dict to improve detect rate.\n",
      "        \n",
      "        Parameter:\n",
      "            - f : A plain text file contains words and their ocurrences.\n",
      "                  Can be a file-like object, or the path of the dictionary file,\n",
      "                  whose encoding must be utf-8.\n",
      "        \n",
      "        Structure of dict file:\n",
      "        word1 freq1 word_type1\n",
      "        word2 freq2 word_type2\n",
      "        ...\n",
      "        Word type may be ignored\n",
      "    \n",
      "    log(...)\n",
      "        log(x[, base])\n",
      "        \n",
      "        Return the logarithm of x to the given base.\n",
      "        If the base not specified, returns the natural logarithm (base e) of x.\n",
      "    \n",
      "    md5 = openssl_md5(...)\n",
      "        Returns a md5 hash object; optionally initialized with a string\n",
      "    \n",
      "    setLogLevel(log_level)\n",
      "    \n",
      "    set_dictionary(self, dictionary_path) method of Tokenizer instance\n",
      "    \n",
      "    suggest_freq(self, segment, tune=False) method of Tokenizer instance\n",
      "        Suggest word frequency to force the characters in a word to be\n",
      "        joined or splitted.\n",
      "        \n",
      "        Parameter:\n",
      "            - segment : The segments that the word is expected to be cut into,\n",
      "                        If the word should be treated as a whole, use a str.\n",
      "            - tune : If True, tune the word frequency.\n",
      "        \n",
      "        Note that HMM may affect the final result. If the result doesn't change,\n",
      "        set HMM=False.\n",
      "    \n",
      "    tokenize(self, unicode_sentence, mode=u'default', HMM=True) method of Tokenizer instance\n",
      "        Tokenize a sentence and yields tuples of (word, start, end)\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: the str(unicode) to be segmented.\n",
      "            - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "            - HMM: whether to use the Hidden Markov Model.\n",
      "\n",
      "DATA\n",
      "    DEFAULT_DICT = None\n",
      "    DEFAULT_DICT_NAME = u'dict.txt'\n",
      "    DICT_WRITING = {}\n",
      "    PY2 = True\n",
      "    __license__ = u'MIT'\n",
      "    __version__ = u'0.38'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    default_encoding = 'utf-8'\n",
      "    default_logger = <logging.Logger object>\n",
      "    dt = <Tokenizer dictionary=None>\n",
      "    log_console = <logging.StreamHandler object>\n",
      "    pool = None\n",
      "    re_eng = <_sre.SRE_Pattern object>\n",
      "    re_han_cut_all = <_sre.SRE_Pattern object>\n",
      "    re_han_default = <_sre.SRE_Pattern object>\n",
      "    re_skip_cut_all = <_sre.SRE_Pattern object>\n",
      "    re_skip_default = <_sre.SRE_Pattern object>\n",
      "    re_userdict = <_sre.SRE_Pattern object>\n",
      "    string_types = (<type 'str'>, <type 'unicode'>)\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    user_word_tag_tab = {}\n",
      "\n",
      "VERSION\n",
      "    0.38\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_list = jieba.cut('钢琴曲欣赏100首',cut_all = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object cut at 0x122dea8c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/py/0gqprtwx2wd_zvrlz6hlq2r40000gn/T/jieba.cache\n",
      "Loading model cost 0.414 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "钢琴/ 钢琴曲/ 琴曲/ 欣赏/ 100/ 首\n"
     ]
    }
   ],
   "source": [
    "print '/ '.join(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_list = jieba.cut_for_search('属羊和属鸡的配吗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "属/ 羊/ 和/ 属鸡/ 的/ 配/ 吗\n"
     ]
    }
   ],
   "source": [
    "print '/ '.join(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss_list = jieba.cut_for_search('一个月的宝宝眼睫毛那么是黄色')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个/ 一个月/ 的/ 宝宝/ 睫毛/ 眼睫毛/ 那么/ 是/ 黄色\n"
     ]
    }
   ],
   "source": [
    "print '/ '.join(ss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage:    python extract_tags.py [file name] -k [top k]\n",
      "\n",
      "__main__.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from optparse import OptionParser\n",
    "\n",
    "USAGE = \"usage:    python extract_tags.py [file name] -k [top k]\"\n",
    "\n",
    "parser = OptionParser(USAGE)\n",
    "parser.add_option(\"-k\", dest=\"topK\")\n",
    "opt, args = parser.parse_args()\n",
    "\n",
    "\n",
    "if len(args) < 1:\n",
    "    print(USAGE)\n",
    "    sys.exit(1)\n",
    "\n",
    "file_name = args[0]\n",
    "\n",
    "if opt.topK is None:\n",
    "    topK = 10\n",
    "else:\n",
    "    topK = int(opt.topK)\n",
    "\n",
    "content = open(file_name, 'rb').read()\n",
    "\n",
    "tags = jieba.analyse.extract_tags(content, topK=topK)\n",
    "\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Crayon_277/Develop/Project/Python/data-analysis/iris\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from optparse import OptionParser\n",
    "import jieba.analyse\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage:    python extract_tags.py [file name] -k [top k]\n",
      "\n",
      "__main__.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "USAGE = \"usage:    python extract_tags.py [file name] -k [top k]\"\n",
    "\n",
    "parser = OptionParser(USAGE)\n",
    "parser.add_option(\"-k\", dest=\"topK\")\n",
    "opt, args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OptionParser in module optparse:\n",
      "\n",
      "class OptionParser(OptionContainer)\n",
      " |  Class attributes:\n",
      " |    standard_option_list : [Option]\n",
      " |      list of standard options that will be accepted by all instances\n",
      " |      of this parser class (intended to be overridden by subclasses).\n",
      " |  \n",
      " |  Instance attributes:\n",
      " |    usage : string\n",
      " |      a usage string for your program.  Before it is displayed\n",
      " |      to the user, \"%prog\" will be expanded to the name of\n",
      " |      your program (self.prog or os.path.basename(sys.argv[0])).\n",
      " |    prog : string\n",
      " |      the name of the current program (to override\n",
      " |      os.path.basename(sys.argv[0])).\n",
      " |    description : string\n",
      " |      A paragraph of text giving a brief overview of your program.\n",
      " |      optparse reformats this paragraph to fit the current terminal\n",
      " |      width and prints it when the user requests help (after usage,\n",
      " |      but before the list of options).\n",
      " |    epilog : string\n",
      " |      paragraph of help text to print after option help\n",
      " |  \n",
      " |    option_groups : [OptionGroup]\n",
      " |      list of option groups in this parser (option groups are\n",
      " |      irrelevant for parsing the command-line, but very useful\n",
      " |      for generating help)\n",
      " |  \n",
      " |    allow_interspersed_args : bool = true\n",
      " |      if true, positional arguments may be interspersed with options.\n",
      " |      Assuming -a and -b each take a single argument, the command-line\n",
      " |        -ablah foo bar -bboo baz\n",
      " |      will be interpreted the same as\n",
      " |        -ablah -bboo -- foo bar baz\n",
      " |      If this flag were false, that command line would be interpreted as\n",
      " |        -ablah -- foo bar -bboo baz\n",
      " |      -- ie. we stop processing options as soon as we see the first\n",
      " |      non-option argument.  (This is the tradition followed by\n",
      " |      Python's getopt module, Perl's Getopt::Std, and other argument-\n",
      " |      parsing libraries, but it is generally annoying to users.)\n",
      " |  \n",
      " |    process_default_values : bool = true\n",
      " |      if true, option default values are processed similarly to option\n",
      " |      values from the command line: that is, they are passed to the\n",
      " |      type-checking function for the option's type (as long as the\n",
      " |      default value is a string).  (This really only matters if you\n",
      " |      have defined custom types; see SF bug #955889.)  Set it to false\n",
      " |      to restore the behaviour of Optik 1.4.1 and earlier.\n",
      " |  \n",
      " |    rargs : [string]\n",
      " |      the argument list currently being parsed.  Only set when\n",
      " |      parse_args() is active, and continually trimmed down as\n",
      " |      we consume arguments.  Mainly there for the benefit of\n",
      " |      callback options.\n",
      " |    largs : [string]\n",
      " |      the list of leftover arguments that we have skipped while\n",
      " |      parsing options.  If allow_interspersed_args is false, this\n",
      " |      list is always empty.\n",
      " |    values : Values\n",
      " |      the set of option values currently being accumulated.  Only\n",
      " |      set when parse_args() is active.  Also mainly for callbacks.\n",
      " |  \n",
      " |  Because of the 'rargs', 'largs', and 'values' attributes,\n",
      " |  OptionParser is not thread-safe.  If, for some perverse reason, you\n",
      " |  need to parse command-line arguments simultaneously in different\n",
      " |  threads, use different OptionParser instances.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, usage=None, option_list=None, option_class=<class optparse.Option>, version=None, conflict_handler='error', description=None, formatter=None, add_help_option=True, prog=None, epilog=None)\n",
      " |  \n",
      " |  add_option_group(self, *args, **kwargs)\n",
      " |  \n",
      " |  check_values(self, values, args)\n",
      " |      check_values(values : Values, args : [string])\n",
      " |      -> (values : Values, args : [string])\n",
      " |      \n",
      " |      Check that the supplied option values and leftover arguments are\n",
      " |      valid.  Returns the option values and leftover arguments\n",
      " |      (possibly adjusted, possibly completely new -- whatever you\n",
      " |      like).  Default implementation just returns the passed-in\n",
      " |      values; subclasses may override as desired.\n",
      " |  \n",
      " |  destroy(self)\n",
      " |      Declare that you are done with this OptionParser.  This cleans up\n",
      " |      reference cycles so the OptionParser (and all objects referenced by\n",
      " |      it) can be garbage-collected promptly.  After calling destroy(), the\n",
      " |      OptionParser is unusable.\n",
      " |  \n",
      " |  disable_interspersed_args(self)\n",
      " |      Set parsing to stop on the first non-option. Use this if\n",
      " |      you have a command processor which runs another command that\n",
      " |      has options of its own and you want to make sure these options\n",
      " |      don't get confused.\n",
      " |  \n",
      " |  enable_interspersed_args(self)\n",
      " |      Set parsing to not stop on the first non-option, allowing\n",
      " |      interspersing switches with command arguments. This is the\n",
      " |      default behavior. See also disable_interspersed_args() and the\n",
      " |      class documentation description of the attribute\n",
      " |      allow_interspersed_args.\n",
      " |  \n",
      " |  error(self, msg)\n",
      " |      error(msg : string)\n",
      " |      \n",
      " |      Print a usage message incorporating 'msg' to stderr and exit.\n",
      " |      If you override this in a subclass, it should not return -- it\n",
      " |      should either exit or raise an exception.\n",
      " |  \n",
      " |  exit(self, status=0, msg=None)\n",
      " |  \n",
      " |  expand_prog_name(self, s)\n",
      " |  \n",
      " |  format_epilog(self, formatter)\n",
      " |  \n",
      " |  format_help(self, formatter=None)\n",
      " |  \n",
      " |  format_option_help(self, formatter=None)\n",
      " |  \n",
      " |  get_default_values(self)\n",
      " |  \n",
      " |  get_description(self)\n",
      " |  \n",
      " |  get_option_group(self, opt_str)\n",
      " |  \n",
      " |  get_prog_name(self)\n",
      " |  \n",
      " |  get_usage(self)\n",
      " |  \n",
      " |  get_version(self)\n",
      " |  \n",
      " |  parse_args(self, args=None, values=None)\n",
      " |      parse_args(args : [string] = sys.argv[1:],\n",
      " |                 values : Values = None)\n",
      " |      -> (values : Values, args : [string])\n",
      " |      \n",
      " |      Parse the command-line options found in 'args' (default:\n",
      " |      sys.argv[1:]).  Any errors result in a call to 'error()', which\n",
      " |      by default prints the usage message to stderr and calls\n",
      " |      sys.exit() with an error message.  On success returns a pair\n",
      " |      (values, args) where 'values' is a Values instance (with all\n",
      " |      your option values) and 'args' is the list of arguments left\n",
      " |      over after parsing options.\n",
      " |  \n",
      " |  print_help(self, file=None)\n",
      " |      print_help(file : file = stdout)\n",
      " |      \n",
      " |      Print an extended help message, listing all options and any\n",
      " |      help text provided with them, to 'file' (default stdout).\n",
      " |  \n",
      " |  print_usage(self, file=None)\n",
      " |      print_usage(file : file = stdout)\n",
      " |      \n",
      " |      Print the usage message for the current program (self.usage) to\n",
      " |      'file' (default stdout).  Any occurrence of the string \"%prog\" in\n",
      " |      self.usage is replaced with the name of the current program\n",
      " |      (basename of sys.argv[0]).  Does nothing if self.usage is empty\n",
      " |      or not defined.\n",
      " |  \n",
      " |  print_version(self, file=None)\n",
      " |      print_version(file : file = stdout)\n",
      " |      \n",
      " |      Print the version message for this program (self.version) to\n",
      " |      'file' (default stdout).  As with print_usage(), any occurrence\n",
      " |      of \"%prog\" in self.version is replaced by the current program's\n",
      " |      name.  Does nothing if self.version is empty or undefined.\n",
      " |  \n",
      " |  set_default(self, dest, value)\n",
      " |  \n",
      " |  set_defaults(self, **kwargs)\n",
      " |  \n",
      " |  set_process_default_values(self, process)\n",
      " |  \n",
      " |  set_usage(self, usage)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  standard_option_list = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from OptionContainer:\n",
      " |  \n",
      " |  add_option(self, *args, **kwargs)\n",
      " |      add_option(Option)\n",
      " |      add_option(opt_str, ..., kwarg=val, ...)\n",
      " |  \n",
      " |  add_options(self, option_list)\n",
      " |  \n",
      " |  format_description(self, formatter)\n",
      " |  \n",
      " |  get_option(self, opt_str)\n",
      " |  \n",
      " |  has_option(self, opt_str)\n",
      " |  \n",
      " |  remove_option(self, opt_str)\n",
      " |  \n",
      " |  set_conflict_handler(self, handler)\n",
      " |  \n",
      " |  set_description(self, description)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(OptionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '3. 关键词提取'\n",
    "print '-'*40\n",
    "print ' TF-IDF'\n",
    "print '-'*40\n",
    "\n",
    "s = \"此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。\"\n",
    "for x, w in jieba.analyse.extract_tags(s, withWeight=True):\n",
    "    print '%s %s' % (x, w)\n",
    "\n",
    "print '-'*40\n",
    "print ' TextRank'\n",
    "print '-'*40\n",
    "\n",
    "for x, w in jieba.analyse.textrank(s, withWeight=True):\n",
    "    print '%s %s' % (x, w)\n",
    "\n",
    "print '='*40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

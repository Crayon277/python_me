{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print 'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEFAULT_DICT',\n",
       " 'DEFAULT_DICT_NAME',\n",
       " 'DICT_WRITING',\n",
       " 'PY2',\n",
       " 'Tokenizer',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__license__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__version__',\n",
       " '_compat',\n",
       " '_get_abs_path',\n",
       " '_lcut',\n",
       " '_lcut_all',\n",
       " '_lcut_for_search',\n",
       " '_lcut_for_search_no_hmm',\n",
       " '_pcut',\n",
       " '_pcut_for_search',\n",
       " '_replace_file',\n",
       " 'absolute_import',\n",
       " 'add_word',\n",
       " 'calc',\n",
       " 'cut',\n",
       " 'cut_for_search',\n",
       " 'default_encoding',\n",
       " 'default_logger',\n",
       " 'del_word',\n",
       " 'disable_parallel',\n",
       " 'dt',\n",
       " 'enable_parallel',\n",
       " 'finalseg',\n",
       " 'get_DAG',\n",
       " 'get_FREQ',\n",
       " 'get_dict_file',\n",
       " 'get_module_res',\n",
       " 'initialize',\n",
       " 'iteritems',\n",
       " 'iterkeys',\n",
       " 'itervalues',\n",
       " 'lcut',\n",
       " 'lcut_for_search',\n",
       " 'load_userdict',\n",
       " 'log',\n",
       " 'log_console',\n",
       " 'logging',\n",
       " 'marshal',\n",
       " 'md5',\n",
       " 'os',\n",
       " 'pkg_resources',\n",
       " 'pool',\n",
       " 're',\n",
       " 're_eng',\n",
       " 're_han_cut_all',\n",
       " 're_han_default',\n",
       " 're_skip_cut_all',\n",
       " 're_skip_default',\n",
       " 're_userdict',\n",
       " 'resolve_filename',\n",
       " 'setLogLevel',\n",
       " 'set_dictionary',\n",
       " 'strdecode',\n",
       " 'string_types',\n",
       " 'suggest_freq',\n",
       " 'sys',\n",
       " 'tempfile',\n",
       " 'text_type',\n",
       " 'threading',\n",
       " 'time',\n",
       " 'tokenize',\n",
       " 'unicode_literals',\n",
       " 'user_word_tag_tab']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/py/0gqprtwx2wd_zvrlz6hlq2r40000gn/T/jieba.cache\n",
      "Loading model cost 0.412 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 0.730014270029\n",
      "吉林 0.659038184374\n",
      "置业 0.488713452211\n",
      "万元 0.339272248186\n",
      "增资 0.335824019852\n",
      "4.3 0.254356755381\n",
      "7000 0.254356755381\n",
      "139.13 0.254356755381\n",
      "2013 0.254356755381\n",
      "实现 0.199009799004\n",
      "综合体 0.194803096247\n",
      "经营范围 0.193897572536\n",
      "亿元 0.191442162359\n",
      "在建 0.175418847684\n",
      "全资 0.171801649885\n",
      "注册资本 0.1712441526\n",
      "百货 0.167344600414\n",
      "零售 0.147505711706\n",
      "子公司 0.145960452378\n",
      "营业 0.13920178509\n"
     ]
    }
   ],
   "source": [
    "s = \"此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。\"\n",
    "for x, w in jieba.analyse.extract_tags(s, withWeight=True):\n",
    "    print '%s %s' % (x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吉林 1.0\n",
      "欧亚 0.996689335418\n",
      "置业 0.643436031309\n",
      "实现 0.589860669286\n",
      "收入 0.43677859948\n",
      "增资 0.409990053128\n",
      "子公司 0.356782959477\n",
      "城市 0.349713836674\n",
      "商业 0.34817220716\n",
      "业务 0.309223099262\n",
      "在建 0.307792916403\n",
      "营业 0.303577704932\n",
      "全资 0.303540981053\n",
      "综合体 0.295808691724\n",
      "注册资本 0.290005194641\n",
      "有限公司 0.280783079858\n",
      "零售 0.278836208612\n",
      "百货 0.278165762845\n",
      "开发 0.26934887793\n",
      "经营范围 0.264276217356\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x, w in jieba.analyse.textrank(s, withWeight=True):\n",
    "    print '%s %s' % (x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Tokenizer in module jieba:\n",
      "\n",
      "class Tokenizer(__builtin__.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dictionary=None)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  add_word(self, word, freq=None, tag=None)\n",
      " |      Add a word to dictionary.\n",
      " |      \n",
      " |      freq and tag can be omitted, freq defaults to be a calculated value\n",
      " |      that ensures the word can be cut out.\n",
      " |  \n",
      " |  calc(self, sentence, DAG, route)\n",
      " |  \n",
      " |  check_initialized(self)\n",
      " |  \n",
      " |  cut(self, sentence, cut_all=False, HMM=True)\n",
      " |      The main function that segments an entire sentence that contains\n",
      " |      Chinese characters into seperated words.\n",
      " |      \n",
      " |      Parameter:\n",
      " |          - sentence: The str(unicode) to be segmented.\n",
      " |          - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      " |          - HMM: Whether to use the Hidden Markov Model.\n",
      " |  \n",
      " |  cut_for_search(self, sentence, HMM=True)\n",
      " |      Finer segmentation for search engines.\n",
      " |  \n",
      " |  del_word(self, word)\n",
      " |      Convenient function for deleting a word.\n",
      " |  \n",
      " |  gen_pfdict(self, f)\n",
      " |  \n",
      " |  get_DAG(self, sentence)\n",
      " |  \n",
      " |  get_dict_file(self)\n",
      " |  \n",
      " |  initialize(self, dictionary=None)\n",
      " |  \n",
      " |  lcut(self, *args, **kwargs)\n",
      " |  \n",
      " |  lcut_for_search(self, *args, **kwargs)\n",
      " |  \n",
      " |  load_userdict(self, f)\n",
      " |      Load personalized dict to improve detect rate.\n",
      " |      \n",
      " |      Parameter:\n",
      " |          - f : A plain text file contains words and their ocurrences.\n",
      " |                Can be a file-like object, or the path of the dictionary file,\n",
      " |                whose encoding must be utf-8.\n",
      " |      \n",
      " |      Structure of dict file:\n",
      " |      word1 freq1 word_type1\n",
      " |      word2 freq2 word_type2\n",
      " |      ...\n",
      " |      Word type may be ignored\n",
      " |  \n",
      " |  set_dictionary(self, dictionary_path)\n",
      " |  \n",
      " |  suggest_freq(self, segment, tune=False)\n",
      " |      Suggest word frequency to force the characters in a word to be\n",
      " |      joined or splitted.\n",
      " |      \n",
      " |      Parameter:\n",
      " |          - segment : The segments that the word is expected to be cut into,\n",
      " |                      If the word should be treated as a whole, use a str.\n",
      " |          - tune : If True, tune the word frequency.\n",
      " |      \n",
      " |      Note that HMM may affect the final result. If the result doesn't change,\n",
      " |      set HMM=False.\n",
      " |  \n",
      " |  tokenize(self, unicode_sentence, mode=u'default', HMM=True)\n",
      " |      Tokenize a sentence and yields tuples of (word, start, end)\n",
      " |      \n",
      " |      Parameter:\n",
      " |          - sentence: the str(unicode) to be segmented.\n",
      " |          - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      " |          - HMM: whether to use the Hidden Markov Model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jieba.Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-08bf0a0704d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'钢琴曲欣赏100首'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "jieba.analyse.TextRank('钢琴曲欣赏100首',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "钢琴曲\n",
      "欣赏\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in jieba.analyse.textrank('钢琴曲欣赏100首', withWeight=False):\n",
    "    print '%s' % (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f= open('test.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe5\\x85\\xb3\\xe9\\x94\\xae\\xe6\\x80\\xa7,\\xe8\\xbf\\x99\\xe4\\xba\\x8b\\xe4\\xbb\\x80\\xe4\\xb9\\x88,\\xe6\\x88\\x91\\xe7\\x9a\\x84\\xe5\\xa4\\xa9\\xe5\\x93\\xaa\\n\\xe6\\x88\\x91\\xe6\\x93\\xa6,\\xe4\\xbb\\x80\\xe4\\xb9\\x88\\xe6\\x83\\x85\\xe5\\x86\\xb5,\\xe8\\xbf\\x99\\xe4\\xb8\\xaa\\xe4\\xb8\\x96\\xe7\\x95\\x8c\\xe5\\xa5\\xbd\\xe5\\xa4\\x8d\\xe6\\x9d\\x82'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键性,这事什么,我的天哪\n",
      "\n",
      "我擦,什么情况,这个世界好复杂\n"
     ]
    }
   ],
   "source": [
    "for i in f:\n",
    "    \n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f= codecs.open('test.txt',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ff=codecs.open('test_a.txt','w',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in f:\n",
    "    ff.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'00627779E16E7C09B975B2CE13C088CB', u'4', u'2', u'0', u'\\u94a2\\u7434\\u66f2\\u6b23\\u8d4f100\\u9996', u'\\u4e00\\u4e2a\\u6708\\u7684\\u5b9d\\u5b9d\\u773c\\u776b\\u6bdb\\u90a3\\u4e48\\u662f\\u9ec4\\u8272', u'\\u5b9d\\u5b9d\\u53f3\\u773c\\u6709\\u773c\\u5c4e', u'\\u5c0f\\u513f\\u62bd\\u6410\\u600e\\u4e48\\u529e', u'\\u5256\\u8179\\u4ea7\\u540e\\u5200\\u53e3\\u4e0a\\u6709\\u7ebf\\u5934', u'\\u5c5e\\u7f8a\\u548c\\u5c5e\\u9e21\\u7684\\u914d\\u5417']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with codecs.open('test.csv',encoding='utf-8') as ipt, codecs.open('test_after.csv','w') as opt:\n",
    "        ope.write('\\xEF\\xBB\\xBF')\n",
    "        opt.write('id,age,gender,education,query_list\\n')\n",
    "        for i in ipt:\n",
    "            seq_list = i.split()\n",
    "            print seq_list\n",
    "            opt.write('{0},{1},{2},{3},{4}'.format(seq_list.pop(0),seq_list.pop(0),seq_list.pop(0),seq_list.pop(0),seq_list))\n",
    "except:\n",
    "    print 'file error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
